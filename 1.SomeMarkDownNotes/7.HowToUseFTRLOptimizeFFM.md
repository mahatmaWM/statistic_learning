---
##如何用FTRL思想来做在线学习的参数估计

---
FTL思想就是每次找到让之前所有损失函数之和最小的参数，FTRL算法就是在FTL的优化目标的基础上，加入了正规化防止过拟合且能产生很好的稀疏解。

http://tech.meituan.com/online-learning.html 这篇文章对思想介绍的比较好。

http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm

L1正则理论上可以得到稀疏解，但是实际上由于是浮点运算，只会无限接近于零而不是绝对等于零，如何将这些接近于0的值变成真正的0从而得到稀疏解，
同时不能太粗暴否则可能会影响模型精度，就是下面这些方法的目的。
要了解这些方法的演变历程，这篇文章不错 http://www.cnblogs.com/EE-NovRain/p/3810737.htm

---
CTR/CVR预估及在线学习以及FTRL(Follow The Regularized Leader)优化参数，这类问题的特点是数据维度特别大，而且很稀疏，所以要求模型要尽可能简单高效。
目前来看还是LR逻辑回归和逻辑回归的升级版Factorization Machines为主，FM主要是在求解二次交叉项系数的时候使用了矩阵因子分解技术为主，都是简单的线性模型。
所以在考虑特征的时候，希望加入一些非线性的因素，比如用树模型得到一些特征，或者离散化特征得到非线性。
[Display Advertising Challenge(CTR预估)](http://blog.csdn.net/hero_fantao/article/details/42747281)
[3 Idiots' Solution & LIBFFM](https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution-libffm)
这两篇文章，作者给出的解决方法就是用gbdt得到一些非线性的特征，然后结合ffm模型，效果比一般的LR好很多。

---
优化这类算法的参数，使用GD梯度下降最多，但是考虑到将其转化为在线学习，那么就有SGD随机梯度下降，这两个方法区别不大，前者对全局数据梯度下降适合线下，后者适合流式更新模型。
但是这种优化方法不能产生稀疏解，为了得到稀疏解同时也要考虑到模型精度问题，从SGD演变出了FOBOS,RDA,以及FTRL等优化算法，其中FTRL是比较好的方法兼顾了稀疏解和精确度，可以理解成兼顾了L1L2正则化。
https://github.com/chenhuang-learn/ftrl 代码实现了google论文中的per-coordinate FTRL_Proximal流程。

---
由LR升级成FM就是考虑了交叉项，相当于增加了很多<x_i、x_j>这样的组合特征，带来的就是参数大量增加，而FM很技巧的处理了这个问题，假设训练样本x_n有n个维度的特征，那么设定一个v_k长度为k的隐向量，n个这样的隐向量来估计所有交叉项的系数。
细节见下面的文章分析。另外在FM的基础上，还有一个FFM模型（第一个F代表field），有人在kaggle上使用过这种方式，也就是将n个维度划分成f个组，这样隐向量会变得更多，nfk个，计算复杂度也比FM更高。

FM：博文http://blog.csdn.net/google19890102/article/details/45532745对FM的原理进行了分析，https://github.com/srendle/libfm是一个cpp实现，
http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html是实际使用的例子。
FFM：http://www.csie.ntu.edu.tw/~r01922136/libffm/是一个cpp实现，原理见论文http://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf

有人用FTRL框架优化FM算法，效果也很好，见http://geek.csdn.net/news/detail/112231