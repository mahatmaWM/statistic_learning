# 组合模型的特点(特别是树模型的组合)

------

DT在分类与回归上的应用很多，理解DT的优缺点[scikit decision tree](http://scikit-learn.org/stable/modules/tree.html)。
从其缺点可以看出，对于bagging、boosting等ensemble方法都是在尝试用不同的方法来解决这些缺点。主要从偏差和方差的角度分析ensemble methods。

对bagging而言：
>* 1、parallel ensemble: each model is built independently。
>* 2、aim to decrease variance, not bias。
>* 3、suitable for high variance low bias models (complex models)。
由于其ensemble方法的出发点是减少方差，所以希望每个基分类器尽量为强分类器复杂一点，它们的偏差小方差大，这样可以做到总体的偏差还比较小，而方差也比较小。而减少总体方差的做法就是尽量让各分类器之间的相关性越小越好，所以可知RF训练每个分类器的时候对样本自助抽样、对特征抽样都是为了使基分类器之间的相关性越小越好。

对boosting而言：
>* 1、sequential ensemble: try to add new models that do well where previous models lack。
>* 2、aim to decrease bias, not variance。
>* 3、suitable for low variance high bias models(simple models)。
由于其ensemble的方法出发点是减少偏差，所以希望每个基分类器尽量为弱分类器简单一点，它们的偏差大方差小，这样可以做到总体的偏差还比较小，而方差也比较小。

对于stacking或者blending的方法，减少方差和偏差都是其目标。做法一般都是两层模型的组合方法，使用较多。
上面讨论的细节见：[Bagging, boosting and stacking in machine learning](http://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)

关于树的组合模型有两种：
>* bagging类的随机森林
>* boosting类的AdaBoost、GBDT

另外还有很有意思一些其他尝试，效果也不错：
>* [Additive Groves](http://additivegroves.net/)(外层采用grove的bagging思想，而每一个grove的创建又借鉴boosting的思想)
>* [DART:Dropouts meet Multiple Additive Regression Trees](https://zhuanlan.zhihu.com/p/24309153)这篇文章在GBDT的基础上提出的改进，利用dropout的思想来调节新生产的每颗树的重要性，而且对dropout比重01的调节可以看到在RF和GBDT之间随机切换，很有意思。

------

随机森林(Random Forest):

    它有很多的优点：
    在数据集上表现良好，在当前的很多数据集上，相对其他算法有着很大的优势。
    它能够处理很高维度（feature很多）的数据，并且不用做特征选择。
    在训练完后，它能够给出哪些feature比较重要。
    在创建随机森林的时候，对generlization error使用的是无偏估计。
    训练速度快，在训练过程中，能够检测到feature间的互相影响。
    容易做成并行化方法，实现比较简单。
    随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。
    在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类的分类算法，然后看看哪一类被选择最多，就预测这个样本为那一类。
    
    在建立每一棵决策树的过程中，有两点需要注意 --- 采样与完全分裂。
    首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。
    对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。
    假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。
    然后进行列采样，从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。
    一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。
    按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。
    我觉得可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。

Gradient Boost其实是一个框架，里面可以套入很多不同的算法。

    Boost是"提升"的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。
    原始的Boost（AdaBoost）算法是在算法开始的时候，为每一个样本赋上一个权重值，初始的时候，大家都是一样重要的。
    在每一步训练中得到的模型，会使得数据点的估计有对有错，我们就在每一步结束后，增加分错的点的权重，减少分对的点的权重，这样使得某些点如果老是被分错，那么就会被“严重关注”，也就被赋上一个很高的权重。
    然后等进行了N次迭代，将会得到N个简单的分类器，然后我们将它们组合起来，得到一个最终的模型。
    
    而Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。
    所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。
    而且这里多说一点，如果第一个基模型很给力，那么后面的模型贡献其实是很小的，容易over-specialization这个方向上走的太远，那么在拟合残差的时候，有人又提出了新花样DART，参看上面的链接。

Additive Groves of Regression Trees（重新整理）。
    
    Grove的意思是果园，该模型顾名思义是将许多回归树的加性模型，也就是许多回归树的集成。
    此处，作者指出，所用的树有与普通的回归树有两点不同：
    1.建立树的时候，选择最佳分叉点的规则不同，文章中用的是RMSE (root mean squared error)；
    2.所有的树不是完全生长的，而是如果一个节点的样本数目少于一定的比例alpha，就停止生长。
    第一点感觉不是很重要，alpha这个参数比较重要，它能用来控制树是否过拟合已经树的复杂性，和建立Grove有关哦。
    
    好，再来说Grove。
    Grove的建立过程是这样的，在确定alpha以后，一棵一棵的建立树，直到有N棵树，然后继续从第一棵开始更新树：
    1.第一棵树利用所有的样本进行训练；
    2.第n棵树用前n-1棵树分错的样本进行训练，直到有N棵树；
    3.从第一棵开始重新训练，样本是其他树分错的样本，循环训练直至前后两轮的训练，效果相近；
    当然，我只介绍了思想，文章中有很多术语；
    层级训练，因为alpha不能太大，这样树太简单，错误率太高；也不能太小，这样第一棵树就能很好的拟合数据了。所以，alpha的值可以逐渐增大，这样，树从简单到复杂，层级训练，来寻求一个较优的解；
    毕竟，寻找alpha和N也是一个难解的问题，所以文章中给出了动态规划方法。
    bagged additive grove:利用bagging的方法继续融合，跟random forest的思想就类似了。
    additive Grove 和 random forest 的区别：
    1.random forest中的回归树是充分生长的，additive Grove中的树控制了复杂度，相当于有剪枝，不至于过拟合；
    2.random forest利用了部分特征进行训练，additive Grove中利用了特征的全集；
    3.一个是果园的集成，一个是一系列利用部分属性充分生长的回归树的集成。
    感觉关键区别在这里，果园的训练考虑了被分错的样本，而随机森林没有。不知如果果园的训练也采用部分特征，会是怎样的结果。

    AG作者在特定数据集上做过如下精度对比，AG是排名第一位。
    gradient groves		  0.909
    
    Boosted Trees         0.899
    
    Random Forest         0.896
    
    Bagged Trees          0.885
    
    SVMs                  0.869
    
    Neural Networks       0.844
    
    K-Nearest Neighbors   0.811
    
    Boosted Stumps        0.792
    
    Decision Trees        0.698
    
    Logistic Regression   0.697
    
    Naive Bayes           0.664
    
    
    
见Appendix中的例子演示如何使用blend技术，对于维度为m*n的训练数据。

第一层使用k=4个不同的分类器分别训练模型，并且分别对所有的数据都做一次预测，这样就得到m*k的一个矩阵。
第二层对这个m*k的矩阵再做一次逻辑回归，就相当于对第一层的结果做blending。

有人分享了其他做集成学习的例子，见[Kaggle-Ensemble-Guide](https://github.com/vzhangmeng726/Kaggle-Ensemble-Guide)

[kaggle](https://www.kaggle.com/c/avazu-ctr-prediction/data)上有一个比赛，这个解决方案[kaggle-avazu](https://github.com/owenzhang/kaggle-avazu)用了几种模型组合的方式，效果很好。
