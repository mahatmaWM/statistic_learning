数据预处理的一些文章总结。
>* [干货：结合Scikit-learn介绍几种常用的特征选择方法](http://www.tuicool.com/articles/ieUvaq)
>* [如何在 Kaggle 首战中进入前 10%](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)
>* [样本不平衡的处理方法：重新采样法、代价敏感学习、训练集划分+分类器集成](http://bcmi.sjtu.edu.cn/~blu/papers/2009/Zhi-FeiYe_CAAI-Transactions-on-Intelligent-Systems_2009.pdf)
>* [Welcome to imbalanced-learn documentation](http://contrib.scikit-learn.org/imbalanced-learn/index.html)
>* [Practical Guide to deal with Imbalanced Classification Problems in R](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/)
>* [不均衡学习的抽样方法](http://blog.csdn.net/u011414200/article/details/50664266)
>* [不平衡数据下的机器学习方法简介](http://www.jianshu.com/p/3e8b9f2764c8)
>* [这个项目对特征的预处理可以参考](https://github.com/wepe/PPD_RiskControlCompetition)
>* [知乎里面的一篇较好的总结](https://zhuanlan.zhihu.com/p/25357547)

scikit里面的很多模型也提供了[class_weight](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)这个参数，
它从代价敏感学习的角度解决这个问题。从其代码实现可见class_weight会影响sample_weight，进而影响损失函数的值。

以下是自己断断续续总结的一些较为实用的数据预处理方法。

---
##数据特点

四种数据类型
分类数据，nominal：邮编、雇员ID、性别等
分类数据，ordinal：[好、较好、最好]
数值数据，interval：日期，温度等
数值数据，ratio：绝对温度，长度，年龄等

数据集特点
维度 稀疏性 分辨率
数据相似度、相异度、邻近度 TODO？？？

---
## 异常数据
异常值处理一般分为以下几个步骤：异常值检测、异常值筛选、异常值处理。
异常值大概包括缺失值、离群值、重复值、数据不一致。

    1、关于缺失值的检测应该包括：缺失值数量、缺失值比例、缺失值与完整值数据筛选。
    2、箱型图检验离群值。
    3、数据去重与数据分组合并存在一定区别，去重是纯粹的所有变量都是重复的，而数据分组合并可能是因为一些主键的重复。

异常值处理

    常见的异常值处理办法是删除法、替代法（连续变量均值替代、离散变量用众数以及中位数替代）、插补法（回归插补、多重插补）。
    除了直接删除，可以先把异常值变成缺失值、然后进行后续缺失值补齐，mice包。

---
## 单个特征的处理
1、连续特征 
    
    除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。但是有的时候需要将连续特征做一些变化，比如数据变换、离散化等等。
    数据变换：多项式 log对数 exp指数 box-cox：尽量转化成正态分布。
    离散化：考虑因变量信息，用熵的方法处理的监督方法。参考这个项目[mdlp-discretization](https://github.com/hlin117/mdlp-discretization)。非监督方法包括实际操作可采用等间隔、等密度、k-means等。还有用树模型来离散化的，做法：单独用此特征和目标值yy训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。
    这里的离散化处理有很多好处，实际使用线性模型LR时都要先这样处理，相当于人为引入非线性特性，又可以使模型更稳定，相当于海量离散特征+简单模型（少量连续特征+复杂模型）。
    
2、无序特征 
    
    可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。**对于属性值太多的类别特征，直接one-hot会使维度变得异常多，后面专门一个章节讨论这种情况的处理**。
    
3、有序特征
    
    有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad < normal < good。当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：
    |status取值|向量表示|
    |---|---------|
    |bad|(1, 0, 0)|
    |normal|(1, 1, 0)|
    |good|(1, 1, 1)|
    上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。

---
## 特征组合
我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。
这两个特征各自可以离散化为3维和2维的向量。
对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。

---
## 多个特征的处理
主要有filter、wrapper、embedded三种方法。
    
    1、思路是寻找自变量与目标变量之间的关系。
    方差选择法：去掉方差几乎为零的特征，因为它没有区分度。
    相关系数法：
    （ pearson相关系数：只能刻画线性相关
      spearman相关系数：变量间单调相关性
      kendalltau等级相关系数：适用于两个变量均为有序分类
      互信息IM：变量间的相互依赖程度
      距离相关系数：反应变量间的非线性相关或者非单调相关
      MIC这种大统一的相关系数
      http://lagrange.math.siu.edu/Xu-d/rongfan-xu.pdf
    ）
    卡方检验：chi2
    互信息，信息增益：minepy包
    
    2、思路是通过目标函数(AUC/MSE)来决定是否加入一个变量。
    迭代产生特征子集: 完全搜索 启发式搜索 随机搜索。
    
    3、思路是学习器本身的特征选择，用正则化方法和树模型都可以体现出feature的重要性。
    L1：特征选择 L2：防止过拟合 决策树：熵，信息增益。

---
## 特征数据的多重共线性检测
多重共线性是多元回归模型可能存在的一类现象，分为完全共线与近似共线两类。
模型的多个解释变量间出现完全共线性时，模型的参数变得无法估计。更多的情况则是近似共线性，这时由于并不违背所有的基本假定，模型参数的估计仍是无偏、一致且有效的。
但是估计的参数的标准差往往较大而使得t统计值减小，参数的显著性也下降导致某些本应存在于模型中的变量被排除，甚至出现参数正负号方面的一些混乱。
[Multicollinearity](https://onlinecourses.science.psu.edu/stat501/node/343) 
[R利用VIF查看共线性问题的例子](http://blog.csdn.net/jiabiao1602/article/details/39177125)展现了可以用VIF或者COR等方法来查看变量之间的相关性，并在R中如何挑选变量进行回归。

    多重共线性的对策：
    增大样本量，可部分的解决共线性问题。
    采用多种自变量筛选方法相结合的方式，建立一个最优的逐步回归方程。
    从专业的角度加以判断，人为的去除在专业上比较次要的，或者缺失值比较多，测量误差比较大的共线性因子。
    进行主成分分析，用提取的因子代替原变量进行回归分析。
    进行岭回归分析，它可以有效的解决多重共线性问题。
    进行通径分析（Path Analysis），它可以对应自变量间的关系加以精细的刻画。

比如在使用逻辑回归的时候，用树模型衍生出一些非线性特征Facebook-ctr/kaggle-criteo

---
## 数据降维

    PCA：使用前数据需先做归一化处理，0均值1方差，另外还有处理缺失数据的PPCA。
    whitening的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。
    数据的白化必须满足两个条件，一是不同特征相关性最小接近0，二是所有特征的方差相等（不一定为1），所以一般是紧接着pca后使用。
    因子分析：找出内在因子变量。
    LDA：线性判别。

---
## 分类属性过多的类别特征的处理
有时候分类属性过多，如果直接one-hot-code就会导致模型的维度过多，自由度降低，不仅对建模样本提出了更高的要求，还增加了模型参数估计的难度和模型的训练时间。
既然这个问题是分类变量取值过多导致的，那么最直接的解决思路显然就是对类别变量进行合并，减少其属性个数。
大体思想可以参看这篇文章的总结[Strategies to encode categorical variables with many categories](https://www.kaggle.com/c/caterpillar-tube-pricing/forums/t/15748/strategies-to-encode-categorical-variables-with-many-categories)。

一、利用聚类算法进行类别合并
合并不能是主观随意的，而应该是基于定量分析之后的结果。为了便于叙述，不妨假设某个分类型自变量X有A1，A2，…,An等取值，我们可以计算出每个类别中实际y=1的比例，如下：
|X|y=1|y=0|y=1的比例|
|-|-|-|-|
|A1|N11|N12|N11/(N11+N12)|
|A2|N21|N22|N21/(N21+N22)|
|An|Nn1|Nn2|Nn1/(Nn1+Nn2)|
然后就可以利用聚类分析来进行类别的合并了。
第一种思路是采用系统聚类法，将A1，A2，…,An看做聚类的对象，各个类别的Y=1比例作为指标进行聚类，即有n个聚类对象，1个聚类指标。聚类结束之后，将聚为一类的类别进行合并。
第二种思路就是采用有序样品的聚类，将各个类别按照Y=1比例从小到大的顺序进行排列，再利用Fisher算法将Y=1比例相近的类别进行合并。这样做最大的好处是我们可以根据信息损失，得到最优的类别个数和相应的合并办法。

二、利用决策树进行类别合并
利用决策树进行类别合并时，首先需要选择一个衡量分类变量之间相关性或影响程度的指标，我们可以使用在前期的文章中曾经介绍过IV信息量或一致性比率。这种类别合并的基本步骤可以表示为：
1、将各个类别按照Y=1比例从小到大的顺序进行排列，并将所有类别视为一个组；
2、利用IV信息量（或者一致性比率），找出最优的二元分割方法，使得被选中的分组方案是所有分组方案中预测能力最强的，这样将所有类别分成了两组，不妨假设为组1和组2；
3、将组1按照上一步同样的步骤分为组11和组12，组2分为组21和组22。再利用IV信息量比较组1和组2的最优分组的预测能力大小，取预测能力最强的组进行分裂，这样将所有类别分成了三组，不妨假设为组1、组2和组3。
然后，按照第3步的做法不断分裂下去，直到分裂形成的组数达到预先设定的个数为止。
**如果因变量是二分变量，可以使用IV信息量或一致性比率；如果因变量的取值个数大于2，那么就可以使用一致性比率来进行预测力的判断。还有一种做法就是直接利用树模型来挑选重要的A1，A2，…,Am。将An个变量热编码，然后运行树模型，挑出重要的feature，剩下的分为其他。**

三、WOE编码
WOE就是所谓的证据权重(weight of evidence)，该方法计算出分类变量每一个类别的WOE值，这样就可以用这个WOE值组成的新变量来替代原来的分类变量。由于新变量是数值型变量，因此该方法实际是将分类变量转化为数值变量，不用再生成虚拟变量，避免了由此产生的维度过多的问题。我们用下面的表格来表示WOE值的计算过程:

|X|y=1|y=0|y=1比例|y=0比例|WOE|
|-|-|-|-|-|-|
|A1|N11|N12|N11/N1|N12/N2|ln[(N11/N1)/(N12/N2)]|
|A1|N21|N22|N21/N1|N22/N2|ln[(N21/N1)/(N22/N2)]|
|An|Nn1|Nn2|Nn1/N1|Nn1/N2|ln[(Nn1/N1)/(Nn2/N2)]|
|sum|N1|N2||

注：ln表示自然对数函数。
从上表可以看出，WOE值实际上是该类别中Y=1与Y=0比例之比的自然对数。需要注意的是，该表的Y=1比例与上一张表的Y=1比例计算方式是不一样的，上一张表的Y=1比例是该类别中Y=1观测个数与该类别所有观测个数之比，而这一张表示该类别Y=1观测个数与样本中所有Y=1观测个数之比。WOE编码法在利用logistic模型建立信用评分卡时应用较多。

四、小结
背后的思想：Generally the logic of the categorical count transformation lies in the fact that features with similar frequencies tend to behave similarly. 比较而言，前两种类别合并的方法，虽然减少了类别个数，但是仍然需要生成若干二分变量；当使用一致性比率进行预测力判断时，第二种方法也适用于多分类模型；第三种方法将分类变量直接转化为数值型变量，模型简洁，易于操作。但是如果出现类别中Y=1或者Y=0个数为零的情况，将导致WOE值无法计算。因此，也可以将两种方法结合起来，先进行简单的类别合并，避免Y=1或者Y=0个数为零的情况，然后再进行WOE编码。

五、其他的方法。
下面是OwenZhang在比赛中使用的Owen's leave one out encoding of categorical variables。
|split|userid|Y|mean(Y)|random|EXP_UID|
|-|-|-|-|-|-|
|training|A1|0|0.667|1.05|0.70035|
|training|A1|1|0.333|0.97|0.32301|
|training|A1|1|0.333|0.98|0.32634|
|training|A1|0|0.667|1.02|0.68034|
|test|A1|-|0.5|1|0.5|
|test|A1|-|0.5|1|0.5|
|training|A2|0|-|-|-|
解释如下：The training set has four rows for User ID "A1". For the first row, the mean value of the dependent variable Y for the other three rows is 2/3 (2 values of 1 and 1 of 0). Multiply this value by a random component, and put the result in the Exp_UID column. For the test data, use the overall mean value of Y from the training data (2 values of 0 and 2 of 1 for A1 gives 0.5).
About random noise, do you think my assumption is right. mean of noise = 1; standard deviation of noise = 0.05 to 0.10