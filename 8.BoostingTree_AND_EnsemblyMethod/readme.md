本节实现一个gbdt，代码参考了https://github.com/bluekingsong/simple-gbdt 原理细节参考fredman那篇论文，也就是本目录的treebst.pdf。
另外给了一个模型blending的例子。

关于gbdt与xgbdt的区别：
1. xgboost在迭代优化的时候使用了目标函数的泰勒展开的二阶近似，paper中说能加快优化的过程，xgboost可自定义目标函数，但是目标函数必须二阶可导也是因为这个；而GDBT中只用了一阶导数。
2. xgboost在目标函数中加入了正则化项，传统的GDBT没有，但可以自己实现。
3. xgboost寻找最佳分割点时，考虑到传统贪心法效率比较低，实现了一种近似贪心法，除此之外还考虑了稀疏数据集、缺失值的处理，这能大大提升算法的效率。
4. xgboost在算法实现时做了很多优化，大大提升了算法的效率。
5. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

问题1：为何gbdt不直接利用负梯度来调节，而是需要用一个分类器来拟合下降的梯度？
    因为这里的负梯度是在训练集上求出的，不能被泛化测试集中。我们的参数是在一个函数空间里面，不能使用例如SGD这样的求解方式。使用一个分类器来拟合，是一个泛化的方式。
