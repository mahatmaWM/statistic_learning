    模型、策略、算法
    模型既是要学习的条件概率分布或者决策函数。
    策略就是经验风险和结构风险的条件下选一个最优化的目标。
        经验风险有0-1损失、平方损失、绝对损失、指数损失、对数损失（又名交叉熵损失函数）。
        结构风险有L1、L2正则化。
        正则化和交叉验证是从模型和数据两方面来选择最好的模型，避免过拟合现象。
    算法就是利用最优化理论求解这个目标，最小二乘、梯度下降、牛顿法、EM迭代。


#### 什么是损失函数
一般来说，我们在进行机器学习任务时，使用的每一个算法都有一个目标函数，算法便是对这个目标函数进行优化，特别是在分类或者回归任务中，便是使用损失函数（Loss Function）作为其目标函数。
损失函数是用来评价模型的预测值$\hat{Y}=f(X)$与真实值$Y$的不一致程度，它是一个非负实值函数。通常使用$L(Y,f(x))$来表示，损失函数越小，模型的性能就越好。
假设有N个样本的样本集为$(X,Y)={(x_i,y_i)}$为样本i的真实值，$\hat{y_i}=f(x_i), i \in [1,N]$为样本i的预测值，f为分类或者回归函数。那么总的损失函数为：
$$L=\sum_{i=1}^{N}\ell(y_i,\hat{y_i})$$

#### 1. 0-1损失，预测值与目标值不相等=1否则=0：
$$
\begin{eqnarray}\ell(y_i,\hat{y_i})=
  \begin{cases}
  1, &y_i \ne \hat{y_i}\cr 0, &y_i = \hat{y_i} \end{cases}
  \end{eqnarray}
$$
但是这个式子不可微分，实际会用以下近似函数来代替。 

#### 2. SVM用的HingeLoss：
$$\ell(y_i,\hat{y_i})=max\\{0,1-y_i \cdot \hat{y_i}\\}$$
$$y_i \in \\{-1,+1\\}$$

#### 3. 回归用的SquareLoss或者AbsoluteLoss：
$$\ell(y_i,\hat{y_i})=(y_i - \hat{y_i})^2$$
$$\ell(y_i,\hat{y_i})=|y_i - \hat{y_i}|$$
$$y_i, \hat{y_i} \in \Re$$

#### 4. boosting算法（AdaBoost）用的ExponentialLoss：
$$\ell(y_i,\hat{y_i})=exp(-y_i \cdot \hat{y_i})$$
$$y_i \in \\{-1,1\\}$$

#### 5. 逻辑回归用的LogLoss或者交叉熵Loss：
LogLoss定义为
$$L(Y,p(y|x))=-log(p(y|x))$$
结合逻辑回归的定义，尽可能使概率P(Y|X)达到最大值，也就是是logloss尽可能小，带入逻辑回归的定义就可以
$$\ell(y_i,\hat{y_i})=y_i \cdot log \hat{y_i} + (1-y_i) \cdot log (1-\hat{y_i})$$
$$y_i \in \\{0,1\\}$$

另外从两个概率分布之间的交叉熵的角度也可以推导同样的结果。
$$CEH(p,q)=E_{p}[-log(q)]=-\sum_{x\in X}p(x)log(q(x))=H(p)+D_{KL}(p||q)$$ 
就是代表了他们之间的KL相似度（p是一个真实分布，是一个定值），而将交叉熵展开就是
$$\ell(y_i,\hat{y_i})=y_i \cdot log \hat{y_i} + (1-y_i) \cdot log (1-\hat{y_i})$$

![equation](http://latex.codecogs.com/gif.latex?\ell(y_i,\hat{y_i})=y_i%20\cdot%20log%20\hat{y_i}%20+%20(1-y_i)%20\cdot%20log%20(1-\hat{y_i}))

![损失函数图](./loss.png)

> * [机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995/)
> * [机器学习中的范数规则化之（二）核范数与规则项参数选择](http://blog.csdn.net/zouxy09/article/details/24972869)