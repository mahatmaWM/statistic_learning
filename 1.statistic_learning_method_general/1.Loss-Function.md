<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

    模型、策略、算法
    模型既是要学习的条件概率分布或者决策函数。
    策略就是经验风险和结构风险的条件下选一个最优化的目标。
        经验风险有0-1损失、平方损失、绝对损失、对数损失；结构风险有L1、L2正则化。
        正则化和交叉验证是从模型和数据两方面来选择最好的模型，避免过拟合现象。
    算法就是利用最优化理论求解这个目标，最小二乘、梯度下降、牛顿法、EM迭代。


#### 什么是损失函数
一般来说，我们在进行机器学习任务时，使用的每一个算法都有一个目标函数，算法便是对这个目标函数进行优化，特别是在分类或者回归任务中，便是使用损失函数（Loss Function）作为其目标函数。
损失函数是用来评价模型的预测值$\hat{Y}=f(X)$与真实值$Y$的不一致程度，它是一个非负实值函数。通常使用$L(Y,f(x))$来表示，损失函数越小，模型的性能就越好。
假设有N个样本的样本集为$(X,Y)={(x_i,y_i)}$为样本i的真实值，$\hat{y_i}=f(x_i), i \in [1,N]$为样本i的预测值，f为分类或者回归函数。那么总的损失函数为：
$$L=\sum_{i=1}^{N}\ell(y_i,\hat{y_i})$$

#### 1. 0-1损失，它是一种较为简单的损失函数，如果预测值与目标值不相等，那么为1，否则为0，即：
$$
\begin{eqnarray}\ell(y_i,\hat{y_i})=
  \begin{cases}
  1, &y_i \ne \hat{y_i}\cr 0, &y_i = \hat{y_i} \end{cases}
  \end{eqnarray}
$$

可以看出上述的定义太过严格，如果真实值为1，预测值为0.999，那么预测应该正确，但是上述定义显然是判定为预测错误，那么可以进行改进为Perceptron Loss。 

#### 2. Perceptron Loss即为感知损失。即：
$$\begin{eqnarray}\ell(y_i,\hat{y_i})=
  \begin{cases}
  1, &|y_i - \hat{y_i}| \gt t\cr 0, &|y_i - \hat{y_i}| \le t\end{cases}
  \end{eqnarray}
  $$
其中t是一个超参数阈值，如在PLA([Perceptron Learning Algorithm,感知机算法](http://kubicode.me/2015/08/06/Machine%20Learning/Perceptron-Learning-Algorithm/))中取t=0.5。

#### 3. Hinge损失可以用来解决间隔最大化问题，如在SVM中解决几何间隔最大化问题，其定义如下：
$$\ell(y_i,\hat{y_i})=max\\{0,1-y_i \cdot \hat{y_i}\\}$$
$$y_i \in \\{-1,+1\\}$$

#### 4. Square Loss即平方误差，常用于回归中。即：
$$\ell(y_i,\hat{y_i})=(y_i - \hat{y_i})^2$$
$$y_i, \hat{y_i} \in \Re$$

#### 5. Absolute Loss即绝对值误差，常用于回归中。即：
$$\ell(y_i,\hat{y_i})=|y_i - \hat{y_i}|$$
$$y_i, \hat{y_i} \in \Re$$

#### 6. Exponential Loss为指数误差，常用于boosting算法中,如[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)
$$\ell(y_i,\hat{y_i})=exp(-y_i \cdot \hat{y_i})$$
$$y_i \in \\{-1,1\\}$$

#### 7. Log Loss在使用似然函数最大化时，其形式是进行连乘，但是为了便于处理，一般会套上log，这样便可以将连乘转化为求和，由于log函数是单调递增函数，因此不会改变优化结果。因此log类型的损失函数也是一种常见的损失函数，如在LR中使用交叉熵(Cross Entropy)作为其损失函数。即：
$$\ell(y_i,\hat{y_i})=y_i \cdot log \hat{y_i} + (1-y_i) \cdot log (1-\hat{y_i})$$
$$y_i \in \\{0,1\\}$$


> * [机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995/)
> * [机器学习中的范数规则化之（二）核范数与规则项参数选择](http://blog.csdn.net/zouxy09/article/details/24972869)
> * [机器学习-损失函数](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/1.Loss-Function.md)