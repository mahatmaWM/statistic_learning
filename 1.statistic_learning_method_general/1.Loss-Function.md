    模型、策略、算法
    模型既是要学习的条件概率分布或者决策函数。
    策略就是经验风险和结构风险的条件下选一个最优化的目标。
        经验风险有0-1损失、平方损失、绝对损失、指数损失、对数损失（又名交叉熵损失函数）。
        结构风险有L1、L2正则化。
        正则化和交叉验证是从模型和数据两方面来选择最好的模型，避免过拟合现象。
    算法就是利用最优化理论求解这个目标，最小二乘、梯度下降、牛顿法、EM迭代。


0.什么是损失函数
每一个算法都有一个目标函数，算法便是对这个目标函数进行优化，特别是在分类或者回归任务中，便是使用损失函数（Loss Function）作为其目标函数，损失函数越小，模型的性能就越好。

1.01损失，预测值与目标值不相等=1否则=0：

![equation](http://latex.codecogs.com/gif.latex?\begin{eqnarray}\ell(y_i,\hat{y_i})=%20\begin{cases}%201,%20&y_i%20\ne%20\hat{y_i}\cr%200,%20&y_i%20=%20\hat{y_i}%20\end{cases}%20\end{eqnarray})
但是这个式子不可微分，实际会用以下近似函数来代替。 

2.SVM用的HingeLoss：

![equation](http://latex.codecogs.com/gif.latex?\ell(y_i,\hat{y_i})=max\(0,1-y_i%20\cdot%20\hat{y_i}\))
其中![equation](http://latex.codecogs.com/gif.latex?y_i%20\in%20\{-1,+1\})

3.回归用的SquareLoss或者AbsoluteLoss：

![equation](http://latex.codecogs.com/gif.latex?\ell(y_i,\hat{y_i})=(y_i%20-%20\hat{y_i})^2=|y_i%20-%20\hat{y_i}|)
其中![equation](http://latex.codecogs.com/gif.latex?y_i,%20\hat{y_i}%20\in%20\Re)

4.boosting算法（AdaBoost）用的ExponentialLoss：

![equation](http://latex.codecogs.com/gif.latex?\ell(y_i,\hat{y_i})=exp(-y_i%20\cdot%20\hat{y_i}))
其中![equation](http://latex.codecogs.com/gif.latex?y_i%20\in%20\{-1,1\})

5.逻辑回归用的LogLoss或者交叉熵Loss：
LogLoss定义为

![equation](http://latex.codecogs.com/gif.latex?L(Y,p(y|x))=-log(p(y|x)))
结合逻辑回归的定义，尽可能使概率P(Y|X)达到最大值，也就是是logloss尽可能小，带入逻辑回归的定义就可以

![equation](http://latex.codecogs.com/gif.latex?\ell(y_i,\hat{y_i})=y_i%20\cdot%20log%20\hat{y_i}%20+%20(1-y_i)%20\cdot%20log%20(1-\hat{y_i}))
其中![equation](http://latex.codecogs.com/gif.latex?y_i%20\in%20\{0,1\})

另外从两个概率分布之间的交叉熵的角度也可以推导同样的结果。

![equation](http://latex.codecogs.com/gif.latex?CEH(p,q)=E_{p}[-log(q)]=-\sum_{x\in%20X}p(x)log(q(x))=H(p)+D_{KL}(p||q))
就是代表了他们之间的KL相似度（p是一个真实分布，是一个定值），而将交叉熵展开就是

![equation](http://latex.codecogs.com/gif.latex?\ell(y_i,\hat{y_i})=y_i%20\cdot%20log%20\hat{y_i}%20+%20(1-y_i)%20\cdot%20log%20(1-\hat{y_i}))

6.以上损失函数的图
![损失函数图](./loss.png)
