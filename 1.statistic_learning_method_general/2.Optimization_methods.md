使用的较多的是梯度下降优化算法和牛顿法优化算法以及各自的变种。

这篇文章[梯度下降优化算法综述](http://blog.csdn.net/heyongluoyao8/article/details/52478715)总结了最常见的三种方法全量梯度下降、随机梯度下降、小批量梯度下降。
这些方法存在的问题以及各种改进：比如合适的学习步长，动量解决鞍点，每一轮迭代的时候随机打乱样本顺序，每个参数自适应不同的学习速率AdaGrad等等。

由于最基本的newton方法求hessian逆矩阵花销很大，所以衍生了许多拟牛顿的方法，用一个正定矩阵来代替hessian矩阵。
见博文[牛顿法与拟牛顿法学习笔记](http://blog.csdn.net/itplus/article/details/21896453)，其中提到的**L-BFGS**使用最为频繁。
