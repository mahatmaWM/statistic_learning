数据预处理的一些文章总结。
>* [干货：结合Scikit-learn介绍几种常用的特征选择方法](http://www.tuicool.com/articles/ieUvaq)
>* [自己汇总的数据预处理](https://www.zybuluo.com/chriswang/note/612242)
>* [ppca github实现](https://github.com/cangermueller/ppca)
>* [如何在 Kaggle 首战中进入前 10%](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)

样本不平衡的处理方法：重新采样法、代价敏感学习、训练集划分+分类器集成。
[这篇论文](http://bcmi.sjtu.edu.cn/~blu/papers/2009/Zhi-FeiYe_CAAI-Transactions-on-Intelligent-Systems_2009.pdf)在3个实际的不平衡数据集上，发现训练集划分+分类器集成方法能较好地处理不平衡数据集。

>* [Welcome to imbalanced-learn documentation](http://contrib.scikit-learn.org/imbalanced-learn/index.html)
>* [Practical Guide to deal with Imbalanced Classification Problems in R](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/)
>* [不均衡学习的抽样方法](http://blog.csdn.net/u011414200/article/details/50664266)
>* [不平衡数据下的机器学习方法简介](http://www.jianshu.com/p/3e8b9f2764c8)

scikit里面的很多模型也提供了[class_weight](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)这个参数，它从代价敏感学习的角度解决这个问题。
从其代码实现可见class_weight会影响sample_weight，进而影响损失函数的值。